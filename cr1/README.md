# ูุจ ุงุณฺฉุฑูพุฑ ูพุงุชูู

ฺฉ ุงุณฺฉุฑูพุฑ ุณุงุฏู ู ฺฉุงุฑุจุฑุฏ ุจุง ูพุงุชูู ุจุฑุง ุจุงุฒุฏุฏ ู ุงุณุชุฎุฑุงุฌ ุงุทูุงุนุงุช ุงุฒ ุตูุญุงุช ูุจ.

## ูุตุจ ูุงุจุณุชฺฏโูุง

```bash
pip install -r requirements.txt
```

## ูุญูู ุงุณุชูุงุฏู

### ุฑูุด ุงูู: ุงุฌุฑุง ูุณุชูู
```bash
python scraper.py
```

### ุฑูุด ุฏูู: ุงุฌุฑุง ูุซุงู
```bash
python example_multi_visit.py
```

ุณูพุณ ุขุฏุฑุณ ุตูุญู ูุจ ููุฑุฏ ูุธุฑ ุฑุง ูุงุฑุฏ ฺฉูุฏ.

### ุฑูุด ุณูู: ุงุณุชูุงุฏู ุฏุฑ ฺฉุฏ ุฎูุฏ

```python
from scraper import WebScraper

# create scraper
scraper = WebScraper("https://example.com")

# visit page
if scraper.visit_page():
    # get title page
    title = scraper.get_title()
    print(f"ุนููุงู: {title}")
    
    # get links
    links = scraper.get_links()
    print(f"ุชุนุฏุงุฏ ููฺฉโูุง: {len(links)}")
    
    # get images
    images = scraper.get_images()
    print(f"ุชุนุฏุงุฏ ุชุตุงูุฑ: {len(images)}")
    
    # save HTML
    scraper.save_html("output.html")
```

## ุงูฺฉุงูุงุช

- โ ุจุงุฒุฏุฏ ุงุฒ ุตูุญุงุช ูุจ
- โ ุงุณุชุฎุฑุงุฌ ุนููุงู ุตูุญู
- โ ุงุณุชุฎุฑุงุฌ ุชูุงู ููฺฉโูุง
- โ ุงุณุชุฎุฑุงุฌ ุชูุงู ุชุตุงูุฑ
- โ ุงุณุชุฎุฑุงุฌ ูุชู ุตูุญู
- โ ุฐุฎุฑู HTML ุตูุญู
- โ ูุฏุฑุช ุฎุทุงูุง
- โ ุดุจูโุณุงุฒ ูุฑูุฑฺฏุฑ ูุงูุน
- ๐ **ุจุงุฒุฏุฏ ฺูุฏุจุงุฑู ุจุง ุชุงุฎุฑ ุชุตุงุฏู** (ุถุฏ ุฑุจุงุช)
- ๐ **User-Agent ูุง ูุชููุน ู ุชุตุงุฏู**
- ๐ **ุชุงุฑุฎฺู ุจุงุฒุฏุฏูุง**
- ๐ **ุขูุงุฑ ู ฺฏุฒุงุฑุดโฺฏุฑ**

## ูุซุงูโูุง ฺฉุงุฑุจุฑุฏ

### ูุซุงู 1: ุงุณุชุฎุฑุงุฌ ููฺฉโูุง ฺฉ ุตูุญู
```python
scraper = WebScraper("https://news.ycombinator.com")
if scraper.visit_page():
    links = scraper.get_links()
    for link in links:
        print(link)
```

### ูุซุงู 2: ุฐุฎุฑู ูุญุชูุง ุตูุญู
```python
scraper = WebScraper("https://example.com")
if scraper.visit_page():
    scraper.save_html("example.html")
    text = scraper.get_text()
    print(text)
```

### ูุซุงู 3: ุจุงุฒุฏุฏ ฺูุฏุจุงุฑู (ุถุฏ ุฑุจุงุช) ๐
```python
scraper = WebScraper("https://example.com")

# visit 10 time with delay 5 until 15 seconds
stats = scraper.visit_multiple_times(
    count=10,              # number of visits
    min_delay=5,           # minimium delay 
    max_delay=15,          # max delay
    random_agent=True      # use of random User-Agent
)

print(f"ูููู: {stats['success']}/{stats['total']}")
print(f"ูุฑุฎ ููููุช: {stats['success_rate']:.1f}%")
```

## ูฺฺฏโูุง ุถุฏ ุฑุจุงุช ๐ก๏ธ

ุงู ุงุณฺฉุฑูพุฑ ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุดูุงุณุง ุจู ุนููุงู ุฑุจุงุช:

### 1. ุชุงุฎุฑ ุชุตุงุฏู
- ุจู ูุฑ ุจุงุฒุฏุฏ ฺฉ ุชุงุฎุฑ ุชุตุงุฏู ุงุนูุงู ูโุดูุฏ
- ุชุงุฎุฑ ูุงุจู ุชูุธู (ูุซูุงู 3 ุชุง 10 ุซุงูู)
- ุดุจูโุณุงุฒ ุฑูุชุงุฑ ุงูุณุงู

### 2. User-Agent ูุชููุน
- 7 User-Agent ูุฎุชูู ุงุฒ ูุฑูุฑฺฏุฑูุง ูุฎุชูู
- ุงูุชุฎุงุจ ุชุตุงุฏู ุฏุฑ ูุฑ ุฏุฑุฎูุงุณุช
- ุดุงูู Chromeุ Firefoxุ Safariุ Edge

### 3. ูุฏุฑูุง ฺฉุงูู HTTP
- ุดุจูโุณุงุฒ ฺฉุงูู ุฏุฑุฎูุงุณุช ูุฑูุฑฺฏุฑ
- Acceptุ Accept-Languageุ Accept-Encoding
- DNTุ Connectionุ Upgrade-Insecure-Requests

### 4. ุชุงุฑุฎฺู ู ุขูุงุฑ
- ุซุจุช ุชูุงู ุจุงุฒุฏุฏูุง ุจุง ุฒูุงู ุฏูู
- ฺฏุฒุงุฑุด ููููุช/ุดฺฉุณุช
- ูุฑุฎ ููููุช

## ุชูุฌู โ๏ธ

- ุงุฒ ุงู ุงุณฺฉุฑูพุฑ ููุท ุจุฑุง ุณุงุชโูุง ุงุณุชูุงุฏู ฺฉูุฏ ฺฉู ุงุฌุงุฒู ุงุณฺฉุฑูพ ูโุฏููุฏ
- ุจู ููุงูู `robots.txt` ุณุงุชโูุง ุงุญุชุฑุงู ุจฺฏุฐุงุฑุฏ
- ุงุฒ ููุงุตู ุฒูุงู ููุงุณุจ ุจู ุฏุฑุฎูุงุณุชโูุง ุงุณุชูุงุฏู ฺฉูุฏ
- ุงุณุชูุงุฏู ูุงุฏุฑุณุช ููฺฉู ุงุณุช ููุฌุฑ ุจู ูุณุฏูุฏ ุดุฏู IP ุดูุง ุดูุฏ
